[2024-05-28T11:17:42.898+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-28T11:17:42.928+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T11:17:43.146+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T11:17:43.146+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-28T11:17:43.169+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): data_extraction> on 2024-05-27 00:00:00+00:00
[2024-05-28T11:17:43.178+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=65) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-05-28T11:17:43.180+0000] {standard_task_runner.py:63} INFO - Started process 67 to run task
[2024-05-28T11:17:43.180+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'data_extraction', 'scheduled__2024-05-27T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/sales_data_pipeline.py', '--cfg-path', '/tmp/tmp4zfgla4c']
[2024-05-28T11:17:43.182+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask data_extraction
[2024-05-28T11:17:43.229+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [running]> on host b594ee19424a
[2024-05-28T11:17:43.338+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud Mamdoh' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='data_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-27T00:00:00+00:00'
[2024-05-28T11:17:43.339+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-28T11:17:43.359+0000] {sales_data_pipeline.py:45} INFO - data_extraction started
[2024-05-28T11:17:43.364+0000] {sales_data_pipeline.py:49} ERROR - Error in data_extraction
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/compat/_optional.py", line 132, in import_optional_dependency
    module = importlib.import_module(name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1324, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'openpyxl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/airflow/dags/sales_data_pipeline.py", line 46, in data_extraction
    process_Ecommrese_data()
  File "/opt/airflow/dags/scripts/transform_data.py", line 7, in process_Ecommrese_data
    df = pd.read_excel('./datalake/bronze/E-commerce.xlsx')
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py", line 504, in read_excel
    io = ExcelFile(
         ^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py", line 1580, in __init__
    self._reader = self._engines[engine](
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py", line 552, in __init__
    import_optional_dependency("openpyxl")
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/compat/_optional.py", line 135, in import_optional_dependency
    raise ImportError(msg)
ImportError: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.
[2024-05-28T11:17:43.367+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-28T11:17:43.368+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-28T11:17:43.377+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=sales_pipeline, task_id=data_extraction, run_id=scheduled__2024-05-27T00:00:00+00:00, execution_date=20240527T000000, start_date=20240528T111742, end_date=20240528T111743
[2024-05-28T11:17:43.394+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-28T11:17:43.418+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-05-28T11:17:43.419+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-28T11:24:30.236+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-28T11:24:30.452+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T11:24:30.462+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T11:24:30.462+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-28T11:24:30.477+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): data_extraction> on 2024-05-27 00:00:00+00:00
[2024-05-28T11:24:30.486+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=65) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-05-28T11:24:30.487+0000] {standard_task_runner.py:63} INFO - Started process 67 to run task
[2024-05-28T11:24:30.488+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'data_extraction', 'scheduled__2024-05-27T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/sales_data_pipeline.py', '--cfg-path', '/tmp/tmpspiodoa0']
[2024-05-28T11:24:30.490+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask data_extraction
[2024-05-28T11:24:30.551+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [running]> on host 57dc34eafbbe
[2024-05-28T11:24:30.671+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud Mamdoh' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='data_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-27T00:00:00+00:00'
[2024-05-28T11:24:30.672+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-28T11:24:30.689+0000] {sales_data_pipeline.py:45} INFO - data_extraction started
[2024-05-28T11:24:30.696+0000] {sales_data_pipeline.py:49} ERROR - Error in data_extraction
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/compat/_optional.py", line 132, in import_optional_dependency
    module = importlib.import_module(name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1324, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'openpyxl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/airflow/dags/sales_data_pipeline.py", line 46, in data_extraction
    process_Ecommrese_data()
  File "/opt/airflow/dags/scripts/transform_data.py", line 7, in process_Ecommrese_data
    df = pd.read_excel('./datalake/bronze/E-commerce.xlsx')
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py", line 504, in read_excel
    io = ExcelFile(
         ^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_base.py", line 1580, in __init__
    self._reader = self._engines[engine](
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py", line 552, in __init__
    import_optional_dependency("openpyxl")
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/compat/_optional.py", line 135, in import_optional_dependency
    raise ImportError(msg)
ImportError: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.
[2024-05-28T11:24:30.699+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-28T11:24:30.699+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-28T11:24:30.712+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=sales_pipeline, task_id=data_extraction, run_id=scheduled__2024-05-27T00:00:00+00:00, execution_date=20240527T000000, start_date=20240528T112430, end_date=20240528T112430
[2024-05-28T11:24:30.741+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-28T11:24:30.770+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-05-28T11:24:30.772+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-28T17:01:59.326+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-28T17:01:59.349+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T17:01:59.357+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T17:01:59.357+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-28T17:01:59.368+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): data_extraction> on 2024-05-27 00:00:00+00:00
[2024-05-28T17:01:59.377+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=54) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-05-28T17:01:59.378+0000] {standard_task_runner.py:63} INFO - Started process 56 to run task
[2024-05-28T17:01:59.378+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'data_extraction', 'scheduled__2024-05-27T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/sales_data_pipeline.py', '--cfg-path', '/tmp/tmpla6n4ska']
[2024-05-28T17:01:59.380+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask data_extraction
[2024-05-28T17:01:59.391+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2024-05-28T17:01:59.423+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [running]> on host 0fa044a0c643
[2024-05-28T17:01:59.500+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud Mamdoh' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='data_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-27T00:00:00+00:00'
[2024-05-28T17:01:59.501+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-28T17:01:59.515+0000] {sales_data_pipeline.py:45} INFO - data_extraction started
[2024-05-28T17:02:10.175+0000] {sales_data_pipeline.py:49} ERROR - Error in data_extraction
Traceback (most recent call last):
  File "/opt/airflow/dags/sales_data_pipeline.py", line 46, in data_extraction
    process_Ecommrese_data()
  File "/opt/airflow/dags/scripts/transform_data.py", line 10, in process_Ecommrese_data
    df['Order Date'] = pd.to_datetime([df['Order Date']])
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/tools/datetimes.py", line 1100, in to_datetime
    result = convert_listlike(argc, format)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/tools/datetimes.py", line 395, in _convert_listlike_datetimes
    raise TypeError(
TypeError: arg must be a string, datetime, list, tuple, 1-d array, or Series
[2024-05-28T17:02:10.180+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-28T17:02:10.181+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-28T17:02:10.189+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=sales_pipeline, task_id=data_extraction, run_id=scheduled__2024-05-27T00:00:00+00:00, execution_date=20240527T000000, start_date=20240528T170159, end_date=20240528T170210
[2024-05-28T17:02:10.239+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-28T17:02:10.257+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-05-28T17:02:10.259+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-28T19:07:57.996+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-28T19:07:58.019+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T19:07:58.025+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T19:07:58.026+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-28T19:07:58.037+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): data_extraction> on 2024-05-27 00:00:00+00:00
[2024-05-28T19:07:58.045+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=54) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-05-28T19:07:58.046+0000] {standard_task_runner.py:63} INFO - Started process 56 to run task
[2024-05-28T19:07:58.046+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'data_extraction', 'scheduled__2024-05-27T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/sales_data_pipeline.py', '--cfg-path', '/tmp/tmpr0wchzid']
[2024-05-28T19:07:58.049+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask data_extraction
[2024-05-28T19:07:58.059+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2024-05-28T19:07:58.088+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [running]> on host 620f12c2bede
[2024-05-28T19:07:58.159+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud Mamdoh' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='data_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-27T00:00:00+00:00'
[2024-05-28T19:07:58.160+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-28T19:07:58.172+0000] {sales_data_pipeline.py:45} INFO - data_extraction started
[2024-05-28T19:08:09.407+0000] {transform_data.py:28} INFO - ecommerce data processing completed
[2024-05-28T19:08:09.413+0000] {sales_data_pipeline.py:47} INFO - data_extraction done
[2024-05-28T19:08:09.413+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-28T19:08:09.414+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-28T19:08:09.442+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=sales_pipeline, task_id=data_extraction, run_id=scheduled__2024-05-27T00:00:00+00:00, execution_date=20240527T000000, start_date=20240528T190758, end_date=20240528T190809
[2024-05-28T19:08:09.477+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-28T19:08:09.494+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-05-28T19:08:09.495+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-05-28T19:12:27.654+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-28T19:12:27.678+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T19:12:27.688+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [queued]>
[2024-05-28T19:12:27.689+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-05-28T19:12:27.706+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): data_extraction> on 2024-05-27 00:00:00+00:00
[2024-05-28T19:12:27.715+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=58) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-05-28T19:12:27.717+0000] {standard_task_runner.py:63} INFO - Started process 61 to run task
[2024-05-28T19:12:27.717+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'data_extraction', 'scheduled__2024-05-27T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/sales_data_pipeline.py', '--cfg-path', '/tmp/tmpp2gnv7i8']
[2024-05-28T19:12:27.720+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask data_extraction
[2024-05-28T19:12:27.731+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2024-05-28T19:12:27.760+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.data_extraction scheduled__2024-05-27T00:00:00+00:00 [running]> on host 90e12a9d37bb
[2024-05-28T19:12:27.854+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud Mamdoh' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='data_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-05-27T00:00:00+00:00'
[2024-05-28T19:12:27.855+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-28T19:12:27.867+0000] {sales_data_pipeline.py:45} INFO - data_extraction started
[2024-05-28T19:12:39.995+0000] {transform_data.py:28} INFO - ecommerce data processing completed
[2024-05-28T19:12:39.999+0000] {sales_data_pipeline.py:47} INFO - data_extraction done
[2024-05-28T19:12:40.000+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-05-28T19:12:40.000+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-28T19:12:40.029+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=sales_pipeline, task_id=data_extraction, run_id=scheduled__2024-05-27T00:00:00+00:00, execution_date=20240527T000000, start_date=20240528T191227, end_date=20240528T191240
[2024-05-28T19:12:40.077+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-28T19:12:40.096+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-05-28T19:12:40.098+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
